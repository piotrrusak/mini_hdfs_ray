# mini_hdfs_ray

A minimalist distributed storage system inspired by HDFS (Hadoop Distributed File System), built in Python using the Ray framework.

---

## ğŸš€ Overview

This project demonstrates the core principles of distributed file systems:
- Splitting data into chunks
- Distributing and replicating chunks across storage nodes
- Monitoring node health and healing data
- Centralized coordination (NameNode)

**Core components:**
- `DataManager`: The central node managing metadata and chunk distribution (like HDFS NameNode)
- `DataSpace`: Worker nodes that store chunks (like HDFS DataNode)
- `AutoHealer`: Background actor that restores lost replicas in case of node failure

---

## ğŸ“ Project Structure

```
mini_hdfs_ray/
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ local_demo.py                   # Demonstration of local system usage
â”‚   â”œâ”€â”€ remote_auto_healer_init.py      # Initialization of auto healer
â”‚   â”œâ”€â”€ remote_demo.py                  # Demonstration of remote system usage
â”‚   â”œâ”€â”€ remote_manager_init.py          # Initialization of manager
â”‚   â”œâ”€â”€ remote_space_init.py            # Initialization of space
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ artifact.py        # Handles metadata of stored artifacts
â”‚   â”œâ”€â”€ auto_healer.py     # Actor that monitors and restores lost data
â”‚   â”œâ”€â”€ data_manager.py    # Main coordination logic for storing and retrieving
â”‚   â”œâ”€â”€ data_space.py      # Simulated storage nodes
â”‚   â””â”€â”€ logger.py          # Central logging setup
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_basic.py      # PyTest tests for key system operations
â”œâ”€â”€ requirements.txt       # Required Python packages
â”œâ”€â”€ README.md              # Project documentation
â””â”€â”€ .gitignore             # Files to ignore in version control
```

---

## ğŸ› ï¸ Installation

```bash
    git clone https://github.com/your-username/mini_hdfs_ray.git
    cd mini_hdfs_ray
    pip install -r requirements.txt
```

---

## â–¶ï¸ Run Local Demo

```bash
    python examples/local_demo.py
```

This runs a sample scenario including:
- Uploading an artifact
- Simulating node failure
- Healing lost replicas
- Retrieving the full data

---

## â–¶ï¸ Run Remote Demo

To run the remote demo, follow the steps below in three separate terminals:

### ğŸ–¥ï¸ Terminal 1 â€“ Launch DataManager

```
    ray start --head --port=6379 --resources='{"manager": 1}'
    python examples/remote_manager_init.py
```

When you launch the DataManager (head node), you will see a log like:

```
    Next steps
      To add another node to this Ray cluster, run
        ray start --address='<HEAD_NODE_IP>:6379'
```

Use this IP address as <HEAD_NODE_IP> in Terminal 2.

### ğŸ–¥ï¸ Terminal 2 â€“ Launch DataSpaces

```
    ray start --address=<HEAD_NODE_IP>:6379 --resources='{"space": 10}'
    python examples/remote_space_init.py
```

### ğŸ–¥ï¸ Terminal 3 â€“ Launch AutoHealer

```
    python examples/remote_auto_healer_init.py
```

### ğŸ–¥ï¸ Terminal 4 â€“ Run the Demo

```
    python examples/remote_demo.py
```

You should see logs about storing and retrieving artifacts from the distributed storage system.

This runs a sample scenario including:
- Uploading an artifact
- Retrieving the full data

The Ray Dashboard is available at:
ğŸ“ http://127.0.0.1:8265

âš ï¸ All remote components (manager and spaces) will automatically terminate after 90 seconds. You can adjust this in the demo scripts.

### ğŸ›‘ Stop the Ray Cluster

After running the remote demo, you may want to stop all Ray processes to free system resources.


Run the following command in any terminal:

```
    ray stop
```

This cleanly stops the Ray runtime.

## âœ… Run Tests

```bash
    pytest
```

---

## ğŸ“š Technologies Used

- Python 3.10
- [Ray](https://docs.ray.io/) â€” Actor-based distributed computing framework
- NumPy â€” Chunking and data manipulation
- PyTest â€” Testing framework
- Logging â€” Configurable system logs

---

## ğŸ’¼ License

This project is licensed under the MIT License.

---

## ğŸ‘¨â€ğŸ’» Author

Feel free to use, adapt or extend it for your own learning or portfolio!
